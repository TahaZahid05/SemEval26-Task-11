{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2026-02-23T12:39:33.754Z",
     "iopub.execute_input": "2026-02-23T12:38:27.552656Z",
     "iopub.status.busy": "2026-02-23T12:38:27.552323Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import os\n",
    "import gc\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoModel, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from torch.amp import autocast, GradScaler\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import shutil\n",
    "from torch.utils.data import Sampler\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "class SyllogismDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.split_pattern = r'[\\.\\u3002\\u0964]+'\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        raw_text = item[\"syllogism\"]\n",
    "        sentences = [s.strip() for s in re.split(self.split_pattern, raw_text) if s.strip()]\n",
    "        if not sentences: sentences = [\"Empty\"]\n",
    "        conclusion = sentences[-1]\n",
    "        premises = sentences[:-1]\n",
    "        premises_text = self.tokenizer.sep_token.join(premises)\n",
    "        encoding = self.tokenizer(premises_text, conclusion, truncation=True, max_length=self.max_len, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        validity = 1.0 if item[\"validity\"] else 0.0\n",
    "        plausibility = 1.0 if item.get(\"plausibility\", False) else 0.0\n",
    "        return {\n",
    "            \"input_ids\": encoding['input_ids'].squeeze(0),\n",
    "            \"attention_mask\": encoding['attention_mask'].squeeze(0),\n",
    "            \"validity_label\": torch.tensor(validity, dtype=torch.float),\n",
    "            \"plausibility_label\": torch.tensor(plausibility, dtype=torch.float),\n",
    "            \"id\": item.get(\"id\", str(idx))\n",
    "        }\n",
    "    \n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0.001, mode='max'):\n",
    "        self.patience, self.min_delta, self.mode = patience, min_delta, mode\n",
    "        self.counter, self.best_score, self.early_stop = 0, None, False\n",
    "    \n",
    "    def __call__(self, score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            return False\n",
    "        improved = score > (self.best_score + self.min_delta) if self.mode == 'max' else score < (self.best_score - self.min_delta)\n",
    "        if improved:\n",
    "            self.best_score, self.counter = score, 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience: self.early_stop = True\n",
    "        return self.early_stop\n",
    "    \n",
    "\n",
    "def compute_semeval_metrics(predictions, ground_truth):\n",
    "    gt_map = {item['id']: item for item in ground_truth}\n",
    "    correct, total = 0, 0\n",
    "    subgroups = {(True, True): [0, 0], (True, False): [0, 0], \n",
    "                 (False, True): [0, 0], (False, False): [0, 0]}\n",
    "    \n",
    "    for pred in predictions:\n",
    "        if pred['id'] not in gt_map: continue\n",
    "        item = gt_map[pred['id']]\n",
    "        pred_val = pred['validity_pred'] if 'validity_pred' in pred else pred.get('validity')\n",
    "        true_val = item['validity']\n",
    "        plaus = item.get('plausibility', False)\n",
    "        \n",
    "        total += 1\n",
    "        if pred_val == true_val: correct += 1\n",
    "            \n",
    "        key = (true_val, plaus)\n",
    "        if key in subgroups:\n",
    "            subgroups[key][1] += 1\n",
    "            if pred_val == true_val: subgroups[key][0] += 1\n",
    "    \n",
    "    accuracy = (correct / total * 100) if total > 0 else 0.0\n",
    "    def get_acc(v, p): return (subgroups[(v, p)][0] / subgroups[(v, p)][1] * 100) if subgroups[(v, p)][1] > 0 else 0.0\n",
    "    \n",
    "    intra_diff = (abs(get_acc(True, True) - get_acc(True, False)) + abs(get_acc(False, True) - get_acc(False, False))) / 2.0\n",
    "    inter_diff = (abs(get_acc(True, True) - get_acc(False, True)) + abs(get_acc(True, False) - get_acc(False, False))) / 2.0\n",
    "    tot_bias = (intra_diff + inter_diff) / 2.0\n",
    "    \n",
    "    ranking_score = accuracy / (1 + math.log(1 + tot_bias))\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"total_bias\": tot_bias,\n",
    "        \"ranking_score\": ranking_score\n",
    "    }\n",
    "\n",
    "def monitor_language_performance(predictions, ground_truth):\n",
    "    pred_map = {p['id']: p for p in predictions}\n",
    "    lang_groups = {}\n",
    "    for item in ground_truth:\n",
    "        lang = item.get('language', 'unknown') \n",
    "        if lang not in lang_groups: lang_groups[lang] = {'gt': [], 'preds': []}\n",
    "        lang_groups[lang]['gt'].append(item)\n",
    "        if item['id'] in pred_map: lang_groups[lang]['preds'].append(pred_map[item['id']])\n",
    "\n",
    "    print(\"-\" * 65)\n",
    "    print(f\"{'LANG':<5} | {'COUNT':<5} | {'ACC':<6} | {'BIAS':<8} | {'SCORE':<7}\")\n",
    "    print(\"-\" * 65)\n",
    "    for lang in sorted(lang_groups.keys()):\n",
    "        group = lang_groups[lang]\n",
    "        if len(group['gt']) == 0: continue\n",
    "        metrics = compute_semeval_metrics(group['preds'], group['gt'])\n",
    "        print(f\"{lang:<5} | {len(group['gt']):<5} | {metrics['accuracy']:<6.2f} | {metrics['total_bias']:<8.4f} | {metrics['ranking_score']:<7.2f}\")\n",
    "    print(\"-\" * 65)\n",
    "\n",
    "def save_model_for_inference(model, tokenizer, save_dir, config_dict):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, 'pytorch_model.bin'))\n",
    "    with open(os.path.join(save_dir, 'config.json'), 'w') as f: json.dump(config_dict, f, indent=2)\n",
    "    tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "class BinarySyllogismModel(nn.Module):\n",
    "    def __init__(self, model_name, dropout_rate=0.1, pooling_type=\"cls\"):\n",
    "        super().__init__()\n",
    "        self.backbone = AutoModel.from_pretrained(model_name)\n",
    "        self.pooling_type = pooling_type\n",
    "        hidden_size = self.backbone.config.hidden_size\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate), nn.Linear(hidden_size, hidden_size), nn.Tanh(), nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        if self.pooling_type == \"mean\":\n",
    "            mask = attention_mask.unsqueeze(-1).expand(outputs.last_hidden_state.size()).float()\n",
    "            vec = torch.sum(outputs.last_hidden_state * mask, 1) / torch.clamp(mask.sum(1), min=1e-9)\n",
    "        else:\n",
    "            vec = outputs.last_hidden_state[:, 0, :]\n",
    "        return self.classifier(vec).squeeze(-1)\n",
    "\n",
    "\n",
    "def train_engine(model, train_loader, val_loader, config, device):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": config['weight_decay']},\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=config['lr'])\n",
    "    \n",
    "    num_update_steps_per_epoch = math.ceil(len(train_loader) / config['acc_steps'])\n",
    "    total_training_steps = num_update_steps_per_epoch * config['epochs']\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(total_training_steps * config['warmup_ratio']), num_training_steps=total_training_steps)\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=config['patience'], min_delta=config['min_delta'], mode='max')\n",
    "    scaler = GradScaler(enabled=config['fp16'])\n",
    "    best_ranking_score = 0.0\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        lambda_schedule = config[\"lambda_schedule\"]\n",
    "        default_lambda = config[\"lambda_default\"]\n",
    "\n",
    "        if epoch < len(lambda_schedule):\n",
    "            current_lambda = lambda_schedule[epoch]\n",
    "        else:\n",
    "            current_lambda = default_lambda\n",
    "\n",
    "        print(f\" Bias Î» (epoch {epoch + 1}): {current_lambda}\")\n",
    "\n",
    "\n",
    "        model.train()\n",
    "        total_train_loss, total_train_bias = 0, 0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{config['epochs']}\")\n",
    "    \n",
    "        for step, batch in enumerate(progress_bar):\n",
    "            input_ids, mask = batch['input_ids'].to(device), batch['attention_mask'].to(device)\n",
    "            v_labels, p_labels = batch['validity_label'].to(device), batch['plausibility_label'].to(device)\n",
    "\n",
    "            with autocast(device_type=\"cuda\", dtype=torch.float16, enabled=config['fp16']):\n",
    "                logits = model(input_ids, mask)\n",
    "                task_loss = nn.BCEWithLogitsLoss()(logits, v_labels)\n",
    "                bias_penalty = torch.tensor(0.0, device=device)\n",
    "                if current_lambda > 0:\n",
    "                    probs = torch.sigmoid(logits)\n",
    "                    conf = torch.where(v_labels == 1, probs, 1.0 - probs)\n",
    "                    intra_diffs = []\n",
    "                    for p in [0, 1]:\n",
    "                        m_p = (p_labels == p)\n",
    "                        if (v_labels[m_p] == 1).any() and (v_labels[m_p] == 0).any():\n",
    "                            intra_diffs.append(torch.abs(conf[m_p][v_labels[m_p]==1].mean() - conf[m_p][v_labels[m_p]==0].mean()))\n",
    "                    intra_loss = torch.stack(intra_diffs).mean() if intra_diffs else torch.tensor(0.0, device=device)\n",
    "                    cross_diffs = []\n",
    "                    for v in [0, 1]:\n",
    "                        m_v = (v_labels == v)\n",
    "                        if (p_labels[m_v] == 1).any() and (p_labels[m_v] == 0).any():\n",
    "                            cross_diffs.append(torch.abs(conf[m_v][p_labels[m_v]==1].mean() - conf[m_v][p_labels[m_v]==0].mean()))\n",
    "                    cross_loss = torch.stack(cross_diffs).mean() if cross_diffs else torch.tensor(0.0, device=device)\n",
    "                    bias_penalty = (intra_loss + cross_loss) / 2.0\n",
    "    \n",
    "                loss = (task_loss + (current_lambda * bias_penalty)) / config['acc_steps']\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            if (step + 1) % config['acc_steps'] == 0 or (step + 1) == len(train_loader):\n",
    "                scaler.unscale_(optimizer); torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer); scaler.update(); scheduler.step(); optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "            total_train_loss += loss.item() * config['acc_steps']\n",
    "            total_train_bias += bias_penalty.item()\n",
    "            progress_bar.set_postfix({'l': f\"{loss.item()*config['acc_steps']:.4f}\", 'b': f\"{bias_penalty.item():.4f}\", 'lr': f\"{scheduler.get_last_lr()[0]:.2e}\"})\n",
    "    \n",
    "        model.eval(); val_predictions, total_val_loss, total_val_bias = [], 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids, mask = batch['input_ids'].to(device), batch['attention_mask'].to(device)\n",
    "                v_labels, p_labels, ids = batch['validity_label'].to(device), batch['plausibility_label'].to(device), batch['id']\n",
    "                with autocast(device_type=\"cuda\", dtype=torch.float16, enabled=config['fp16']):\n",
    "                    logits = model(input_ids, mask)\n",
    "                    total_val_loss += nn.BCEWithLogitsLoss()(logits, v_labels).item()\n",
    "                    probs = torch.sigmoid(logits); conf = torch.where(v_labels == 1, probs, 1.0 - probs)\n",
    "                    \n",
    "                    i_diffs = [torch.abs(conf[p_labels==p][v_labels[p_labels==p]==1].mean() - conf[p_labels==p][v_labels[p_labels==p]==0].mean()) for p in [0,1] if (v_labels[p_labels==p]==1).any() and (v_labels[p_labels==p]==0).any()]\n",
    "                    c_diffs = [torch.abs(conf[v_labels==v][p_labels[v_labels==v]==1].mean() - conf[v_labels==v][p_labels[v_labels==v]==0].mean()) for v in [0,1] if (p_labels[v_labels==v]==1).any() and (p_labels[v_labels==v]==0).any()]\n",
    "                    total_val_bias += ((torch.stack(i_diffs).mean() if i_diffs else torch.tensor(0.0, device=device)) + (torch.stack(c_diffs).mean() if c_diffs else torch.tensor(0.0, device=device))).item() / 2.0\n",
    "                \n",
    "                preds = (probs >= 0.5).long().cpu().numpy()\n",
    "                for i, uid in enumerate(batch['id']): val_predictions.append({'id': uid, 'validity_pred': bool(preds[i])})\n",
    "        \n",
    "        metrics = compute_semeval_metrics(val_predictions, val_loader.dataset.data)\n",
    "        print(f\"\\n Epoch {epoch + 1} | Train Total Loss: {total_train_loss/len(train_loader):.4f} | Train Bias Loss: {total_train_bias/len(train_loader):.4f} | Validation Total Loss: {total_val_loss/len(val_loader):.4f} | Validation Total Bias: {total_val_bias/len(val_loader):.4f}\")\n",
    "        print(f\"    Acc: {metrics['accuracy']:.4f}% | Bias: {metrics['total_bias']:.4f} | Rank: {metrics['ranking_score']:.4f}\")\n",
    "        print(\"\\n    Per-Language Stats:\"); monitor_language_performance(val_predictions, val_loader.dataset.data)\n",
    "    \n",
    "        if metrics['ranking_score'] > (best_ranking_score + config['min_delta']):\n",
    "            best_ranking_score = metrics['ranking_score']\n",
    "            save_model_for_inference(\n",
    "                model, \n",
    "                train_loader.dataset.tokenizer, \n",
    "                config['save_dir'], \n",
    "                {\n",
    "                    'model_name': config['model_name'], \n",
    "                    'max_len': config['max_len'], \n",
    "                    'dropout_rate': config['dropout_rate'],\n",
    "                    'pooling_type': config.get('pooling_type', 'cls'),\n",
    "                    \"lambda_schedule\": config[\"lambda_schedule\"],\n",
    "                    \"lambda_default\": config[\"lambda_default\"]\n",
    "                }\n",
    "            )\n",
    "            print(\"    Best Checkpoint Saved!\")\n",
    "        if early_stopping(metrics['ranking_score']): break\n",
    "        torch.cuda.empty_cache(); gc.collect()\n",
    "    return model\n",
    "\n",
    "def predict(model, dataloader, device, config):\n",
    "    model.eval(); predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Predicting\"):\n",
    "            with autocast(device_type=\"cuda\", dtype=torch.float16, enabled=config['fp16']):\n",
    "                logits = model(batch['input_ids'].to(device), batch['attention_mask'].to(device))\n",
    "            preds = (torch.sigmoid(logits) >= 0.5).long().cpu().numpy()\n",
    "            for i, uid in enumerate(batch['id']): predictions.append({'id': uid, 'validity_pred': bool(preds[i])})\n",
    "    return predictions\n",
    "\n",
    "class StratifiedBatchSampler(Sampler):\n",
    "    def __init__(self, group_ids, batch_size, num_groups=6):\n",
    "        self.group_ids = np.array(group_ids)\n",
    "        self.batch_size = batch_size\n",
    "        self.num_groups = num_groups\n",
    "        \n",
    "        self.groups = defaultdict(list)\n",
    "        for idx, g in enumerate(self.group_ids):\n",
    "            self.groups[g].append(idx)\n",
    "        \n",
    "        self.group_keys = list(self.groups.keys())\n",
    "        \n",
    "        self.samples_per_group = max(1, batch_size // num_groups)\n",
    "        \n",
    "        min_group_size = min(len(v) for v in self.groups.values())\n",
    "        self.num_batches = min_group_size // self.samples_per_group\n",
    "\n",
    "    def __iter__(self):\n",
    "        shuffled_groups = {}\n",
    "        for g in self.group_keys:\n",
    "            shuffled = np.array(self.groups[g]).copy()\n",
    "            np.random.shuffle(shuffled)\n",
    "            shuffled_groups[g] = shuffled\n",
    "        \n",
    "        for i in range(self.num_batches):\n",
    "            batch = []\n",
    "            \n",
    "            for g in self.group_keys:\n",
    "                start = i * self.samples_per_group\n",
    "                end = (i + 1) * self.samples_per_group\n",
    "                batch.extend(shuffled_groups[g][start:end].tolist())\n",
    "            \n",
    "            np.random.shuffle(batch)\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "\n",
    "CONFIG = {\n",
    "    \"model_name\": \"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\",\n",
    "    \"batch_size\": 144, \"epochs\": 3, \"lr\": 2e-05, \"warmup_ratio\": 0.06, \"weight_decay\": 0.01,\n",
    "    \"acc_steps\": 1, \"fp16\": True, \"patience\": 2, \"min_delta\": 0.001,\n",
    "    \"save_dir\": \"/kaggle/working/trained_model_task3\",\n",
    "    \"pooling_type\": \"cls\", \n",
    "    \"lambda_schedule\": [0.5, 1.5, 2.5],\n",
    "    \"lambda_default\": 2.5,\n",
    "    \"dropout_rate\": 0.1,\n",
    "    \"max_len\": 96\n",
    "}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    with open(\"/kaggle/input/task-3-dataset/task_3_final_dataset.json\", 'r') as f: train_aug_data = json.load(f)\n",
    "    with open(\"/kaggle/input/task-3-dataset/curator_combined_dataset.json\", 'r') as f: gold_data = json.load(f)\n",
    "    with open(\"/kaggle/input/task3-hans/task3_dataset_hans_translated.json\", 'r') as f: hans_data = json.load(f)\n",
    "    print(f\" Loaded {len(train_aug_data)} aug samples, {len(gold_data)} gold samples, and {len(hans_data)} hans samples.\")\n",
    "\n",
    "    wrong_ids_lst = [\"9f087d4f-5b6f-4d89-b9a0-2ed39ea55950\", \"213df683-1fcc-4372-9833-3120c641d5eb\", \"49f81753-fe0d-492c-91fc-c565f8e5ee1e\", \"202e8697-32e7-4ac9-ae52-4cf462be9766\", \"e1d810fa-9d81-4a2f-899b-4b9de70ec62d\", \"b9303323-806b-4664-b29a-1a81fe0e7af0\", \"6a2a14d5-6a43-4610-a539-6a5afe905356\", \"080b1667-1426-47c6-95c5-61451e0deee6\", \"191acf6e-20dd-4c59-ae05-db7585ecef52\", \"f0402a7a-f2a2-4430-b18d-fa56ada5acf3\"]\n",
    "\n",
    "    train_aug_data = [e for e in train_aug_data if e.get(\"id\") not in wrong_ids_lst]\n",
    "\n",
    "    if train_aug_data:\n",
    "        random.seed(42); random.shuffle(train_aug_data); random.shuffle(gold_data); random.shuffle(hans_data)\n",
    "        \n",
    "        stratify_labels = [f\"{x['language']}_{x['validity']}_{x.get('plausibility', False)}\" for x in gold_data]\n",
    "\n",
    "        train_gold, temp, _, temp_lbls = train_test_split(gold_data, stratify_labels, test_size=0.20, random_state=42, stratify=stratify_labels)\n",
    "        val_data, test_data = train_test_split(temp, test_size=0.50, random_state=42, stratify=temp_lbls)\n",
    "\n",
    "        train_data = train_aug_data + train_gold + hans_data\n",
    "        \n",
    "        def make_group_id(item):\n",
    "            return f\"{item.get('language','unk')}_{item['validity']}_{item.get('plausibility', False)}\"\n",
    "        \n",
    "        train_group_ids = [make_group_id(x) for x in train_data]\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n",
    "        \n",
    "        train_dataset = SyllogismDataset(train_data, tokenizer, CONFIG['max_len'])\n",
    "        \n",
    "        batch_sampler = StratifiedBatchSampler(\n",
    "            group_ids=train_group_ids,\n",
    "            batch_size=CONFIG['batch_size'],\n",
    "            num_groups=72\n",
    "        )\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_sampler=batch_sampler,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        val_loader = DataLoader(SyllogismDataset(val_data, tokenizer, CONFIG['max_len']), batch_size=CONFIG['batch_size'], shuffle=False, num_workers=4, pin_memory=True)\n",
    "        test_loader = DataLoader(SyllogismDataset(test_data, tokenizer, CONFIG['max_len']), batch_size=CONFIG['batch_size'], shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "        model = BinarySyllogismModel(CONFIG['model_name'], CONFIG['dropout_rate'], CONFIG['pooling_type'])\n",
    "        \n",
    "        model.backbone.config.use_cache = False  \n",
    "        model.backbone.gradient_checkpointing_enable(\n",
    "            gradient_checkpointing_kwargs={\"use_reentrant\": False}\n",
    "        )\n",
    "        \n",
    "        model = model.to(device)\n",
    "        os.makedirs(CONFIG['save_dir'], exist_ok=True)\n",
    "\n",
    "        trained_model = train_engine(model, train_loader, val_loader, CONFIG, device)\n",
    "\n",
    "        print(\"\\n Predicting on Test Set...\")\n",
    "        test_preds = predict(trained_model, test_loader, device, CONFIG)\n",
    "        metrics = compute_semeval_metrics(test_preds, test_data)\n",
    "\n",
    "        print(f\"\\n FINAL: Acc: {metrics['accuracy']:.2f}% | Bias: {metrics['total_bias']:.4f} | Rank: {metrics['ranking_score']:.4f}\")\n",
    "        print(\"\\n Final Per-Language Analysis:\"); monitor_language_performance(test_preds, test_data)\n",
    "\n",
    "        output_base = \"/kaggle/working/subtask_3_bias_scheduler\"\n",
    "        with open(f\"{output_base}_predictions.json\", \"w\") as f: json.dump(test_preds, f, indent=4)\n",
    "        with open(f\"{output_base}_ground_truth.json\", \"w\") as f: json.dump(test_data, f, indent=4)\n",
    "        shutil.make_archive(f\"{output_base}_model\", 'zip', CONFIG['save_dir'])\n",
    "        print(\"\\n Predictions and Model Saved/Zipped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 14822496,
     "datasetId": 8940612,
     "sourceId": 14043773,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 15094318,
     "datasetId": 9122531,
     "sourceId": 14291498,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
